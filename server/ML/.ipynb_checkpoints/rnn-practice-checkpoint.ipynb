{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 477203 characters\n"
     ]
    }
   ],
   "source": [
    "path_to_file = './SMSSpamCollection.txt'\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tOk lar... Joking wif u oni...\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "int_text = np.array([char2idx[char] for char in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "a\n",
      "m\n",
      "\t\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(int_text)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there go'\n",
      "'t amore wat...\\nham\\tOk lar... Joking wif u oni...\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final'\n",
      "\" tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075o\"\n",
      "\"ver18's\\nham\\tU dun say so early hor... U c already then say...\\nham\\tNah I don't think he goes to usf, h\"\n",
      "\"e lives around here though\\nspam\\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g'\n",
      "Target data: 'am\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there go'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 73 ('h')\n",
      "  expected output: 66 ('a')\n",
      "Step    1\n",
      "  input: 66 ('a')\n",
      "  expected output: 78 ('m')\n",
      "Step    2\n",
      "  input: 78 ('m')\n",
      "  expected output: 0 ('\\t')\n",
      "Step    3\n",
      "  input: 0 ('\\t')\n",
      "  expected output: 41 ('G')\n",
      "Step    4\n",
      "  input: 41 ('G')\n",
      "  expected output: 80 ('o')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 118) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predic   tions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           30208     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 118)           120950    \n",
      "=================================================================\n",
      "Total params: 4,089,462\n",
      "Trainable params: 4,089,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  55,  48,  79,   7,  77,  47,  10,  40,  49,  87,  59,  64,\n",
       "        87,  40,  51, 101,  59, 115,  72, 114,  18,  60,  96,  85,  39,\n",
       "        24,  44, 104,  77,  66,  64, 116,  78,   4,  31,  18,  95,  21,\n",
       "        25,  23,  51, 111, 109,  28,   6,  70,  56,  70,  82,  49,  85,\n",
       "        81,  32, 113,  61,  93,   4,  26,  17,  82,  29,  73, 108,  71,\n",
       "         2,  12,  97,  59,  72,  25, 112,  37,  44,   0,  53,  13,  24,\n",
       "        11,  62,  60, 112,  69,  82,  86,  44,  62,  38,   8,  83,  98,\n",
       "       101,  65,  46,  63,  25,  96,  45,  15,  50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"spam\\tCollect your VALENTINE'S weekend to PARIS inc Flight & Hotel + £200 Prize guaranteed! Text: PAR\"\n",
      "\n",
      "Next Char Predictions: \n",
      " '\\nUNn%lM(FOvY^vFQ»Y┾g…0Z\\x93tE6Jèla^〨m\"=0\\x92375Q‘–:$eVeqOtp>“[~\"8/q;hüf *\\x94Yg7’CJ\\tS+6)\\\\Z’dquJ\\\\D&r\\x96»_L]7\\x93K-P'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 118)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.7710176\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 73 steps\n",
      "Epoch 1/10\n",
      "73/73 [==============================] - 109s 1s/step - loss: 3.4503\n",
      "Epoch 2/10\n",
      "73/73 [==============================] - 108s 1s/step - loss: 2.5436\n",
      "Epoch 3/10\n",
      "73/73 [==============================] - 105s 1s/step - loss: 2.3458\n",
      "Epoch 4/10\n",
      "73/73 [==============================] - 105s 1s/step - loss: 2.2069\n",
      "Epoch 5/10\n",
      "73/73 [==============================] - 106s 1s/step - loss: 2.0805\n",
      "Epoch 6/10\n",
      "73/73 [==============================] - 107s 1s/step - loss: 1.9601\n",
      "Epoch 7/10\n",
      "73/73 [==============================] - 104s 1s/step - loss: 1.8487\n",
      "Epoch 8/10\n",
      "73/73 [==============================] - 104s 1s/step - loss: 1.7489\n",
      "Epoch 9/10\n",
      "73/73 [==============================] - 105s 1s/step - loss: 1.6584\n",
      "Epoch 10/10\n",
      "73/73 [==============================] - 104s 1s/step - loss: 1.5779\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            30208     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 118)            120950    \n",
      "=================================================================\n",
      "Total params: 4,089,462\n",
      "Trainable params: 4,089,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     with we gettin caris, my po I'll be\n",
      "ham\tI jook tosts dy at keep late has how it anso.\n",
      "ham\tWell I was to i'm out and af offe ...\n",
      "ham\tCood getting me aft fant i looon?. Who you having down them that ? U can i think of the weeks ok lack, they hows to anytay ok a too patt more . I will take  pint so then cam. I get back?\n",
      "spam\tHey aftoouna sodry out. Still this suz Holiday or ifong day. At you so much i've remam\tshopai it there is you something is ur loah!. My call me... U chenight today. COHAPSONT LOVE 0808684812787 ESCANE! D WHAN ferm) knows i have? And have at already the good here await college\n",
      "ham\tSuve YOU--BIOU. I d onling everything 8007 =fue chat.ha Vadiep) WC lls 89\n",
      "ham\tHe gas same the puying my satch sounds pifing him, to day that Diy u life to noing and they dnow...\n",
      "ham\tU points. Sad a gaol back team\n",
      "ham\tSant that you guys me 4 now.\n",
      "spam\tHearlo! Call 090600191150 nward DI\n",
      "spam\tXHIs to tell there is a $950).. Call 09051202605 \n",
      "ham\tWhat argict you to pale at from 8150min't washing \n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=u\"ham     \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
